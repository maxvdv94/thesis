# -*- coding: utf-8 -*-
"""GANomaly_3_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/150oCxKfk-mdf3ZrC_egbpBoYjkY-HDt7
"""

#mounting drive 
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
## packages
import matplotlib

matplotlib.use('Agg')

from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers import MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.initializers import RandomNormal

from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

## Loading the data
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

## stacking the datasets
X = np.vstack([X_train, X_test])
Y = np.hstack([Y_train, Y_test])

## removing one number as anomaly
def find_number(num):
    for index, label in enumerate(Y_test):
        if label == num:
            return index

for num_rm in range(8):
  X_non = []
  Y_non = []
  X_rm = []
  Y_rm = []
  for i, label in enumerate(Y):
    if label != num_rm:
      X_non.append(X[i])
      Y_non.append(Y[i])
    else:
      X_rm.append(X[i])
      Y_rm.append(Y[i])

  X_non = np.asarray(X_non)
  X_rm = np.asarray(X_rm)

  X_train, X_test, Y_train, Y_test = train_test_split(X_non, Y_non, 
                                                      test_size=0.2, random_state=42)

  X_test = np.vstack([X_test, X_rm])
  Y_test = np.hstack([Y_test, Y_rm])

print(np.unique(Y_train, return_counts=True))
print(np.unique(Y_test, return_counts=True))

print(X_train.shape)
print(len(Y_train))
print(X_test.shape)
print(len(Y_test))

print(type(X_test[0]))

print(np.min(X_train), np.max(X_train))
print(np.min(X_test), np.max(X_test))

plt.imshow(X_train[5],cmap='Greys')

## Scaling the data between -1 & 1
X_train = (X_train.astype(np.float32) -127.5)/127.5
X_test = (X_test.astype(np.float32) -127.5)/127.5

print(np.min(X_train[10]), np.max(X_train[10]))
print(np.min(X_train), np.max(X_train))

print(np.min(X_test[1000]), np.max(X_test[1000]))
print(np.min(X_test), np.max(X_test))

print(np.max(X_train[80] * 127.5 +127.5))

plt.imshow((X_train[81] * 127.5 +127.5).astype(int), cmap='Greys')

plt.imshow((X_test[30] * 127.5 +127.5).astype(int), cmap='Greys')

print(X_train[0].shape)

print(X_train.shape)

latent_dim = 100
input_shape = (28, 28, 1)


def make_encoder():
  modelE = Sequential()
  modelE.add(Conv2D(32, kernel_size=(3, 2), padding="same", input_shape=input_shape))
  modelE.add(BatchNormalization(momentum=0.8))
  modelE.add(Activation('relu'))
  modelE.add(MaxPooling2D(pool_size=(2, 2)))
  modelE.add(Conv2D(64, kernel_size=(3, 2), padding="same"))
  modelE.add(BatchNormalization(momentum=0.8))
  modelE.add(Activation('relu'))
  modelE.add(MaxPooling2D(pool_size=(2, 1)))
  modelE.add(Conv2D(128, kernel_size=(3, 2), padding="same"))
  modelE.add(BatchNormalization(momentum=0.8))
  modelE.add(Activation('relu'))
  modelE.add(Flatten())
  modelE.add(Dense(latent_dim))
  return modelE

# Encoder 1
enc_model_1 = make_encoder()
img = Input(shape=input_shape)
z = enc_model_1(img)
encoder1 = Model(img, z)

enc_model_1.summary()

encoder1.summary()

# Generator
batch_size = 100

modelG = Sequential()
modelG.add(Dense(128 * 7 * 7, input_dim=latent_dim))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(LeakyReLU(alpha=0.2))
modelG.add(Reshape((7, 7, 128)))
modelG.add(Conv2DTranspose(128, kernel_size=(3,2), strides=2, padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(LeakyReLU(alpha=0.2))
modelG.add(Conv2DTranspose(64, kernel_size=(3,2), strides=2, padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(LeakyReLU(alpha=0.2))
modelG.add(Conv2DTranspose(1, kernel_size=(3,2), strides=1, padding="same", activation='tanh'))

modelG.summary()

z = Input(shape=(latent_dim,))
gen_img = modelG(z)
generator = Model(z, gen_img)

generator.summary()

# Encoder 2
enc_model_2 = make_encoder()
img = Input(shape=input_shape)
z = enc_model_2(img)
encoder2 = Model(img, z)

encoder2.summary()

# Discriminator
modelD = Sequential()
modelD.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(28, 28, 1), padding="same"))
modelD.add(LeakyReLU(alpha=0.2))
modelD.add(Dropout(0.2))
modelD.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))
modelD.add(ZeroPadding2D(padding=((0, 1), (0, 1))))
modelD.add(BatchNormalization(momentum=0.8))
modelD.add(LeakyReLU(alpha=0.2))
modelD.add(Dropout(0.2))
modelD.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))
modelD.add(BatchNormalization(momentum=0.8))
modelD.add(LeakyReLU(alpha=0.2))
modelD.add(Dropout(0.2))
#modelD.add(Conv2D(256, kernel_size=3, padding="same"))
#modelD.add(LeakyReLU(alpha=0.2))
modelD.add(Flatten())
modelD.add(Dense(1, activation='sigmoid'))

discriminator = modelD
optimizer = Adam(0.00001, 0.5)

discriminator.summary()

# Build and compile the discriminator
discriminator.compile(loss=['binary_crossentropy'],
                          optimizer=optimizer,
                          metrics=['accuracy'])

discriminator.trainable = False

layer_name = 'conv2d_9'
layer_name_2 = 'conv2d_8'
layer_name_3 = 'conv2d_7'
get_features = Model(inputs=discriminator.input, outputs=discriminator.get_layer(layer_name).output)
get_features_2 = Model(inputs=discriminator.input, outputs=discriminator.get_layer(layer_name_2).output)
get_features_3 = Model(inputs=discriminator.input, outputs=discriminator.get_layer(layer_name_3).output)

batch_size = 100

# Adversarial ground truths
fake = np.zeros((batch_size, 1))
real = np.ones((batch_size, 1))

g_loss_list = []
d_loss_list = []

# First image encoding
img = Input(shape=input_shape)
z = encoder1(img)
f = get_features(img)

# Generate image from encoding
img_ = generator(z)
f_ = get_features(img_)

#second image encoding
z_ = encoder2(img_)
real = discriminator(img_)

# Set up and compile the combined model
# Trains generator to fool the discriminator
# and decrease loss between (img, _img) and (z, z_)
bigan_generator = Model(img, [f_, img_, z_])
bigan_generator.compile(loss=['mean_squared_error', 'mean_absolute_error',
                                  'mean_squared_error'], optimizer=optimizer, loss_weights=[1, 1, 1])

bigan_generator.summary()

X_train = np.expand_dims(X_train, axis=3)
X_test = np.expand_dims(X_test, axis=3)
idx = np.random.randint(0, X_train.shape[0], batch_size)
imgs = X_train[idx]

print(imgs.shape)

z = encoder1.predict(imgs)
imgs_ = generator.predict(z)
real = discriminator.predict(imgs_)
f = get_features.predict(imgs_)

g_loss = bigan_generator.train_on_batch(imgs, [f, imgs, z])

print(g_loss)

print(real.shape)
print(fake.shape)
print(X_train.shape)
print(X_test.shape)
print(len(Y_train))
print(len(Y_test))

print(np.unique(Y_train, return_counts=True))
print(np.unique(Y_test, return_counts=True))

print(real.dtype)
print(fake.dtype)
print(X_train.dtype)
print(X_test.dtype)

## loading the weights to return training
generator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_gen_weights_ganomaly_MNIST.h5')
encoder1.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc1_weights_ganomaly_MNIST.h5')
encoder2.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc2_weights_ganomaly_MNIST.h5')
discriminator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_dis_weights_ganomaly_MNIST.h5')

#idx = np.random.randint(0, X_train.shape[0], batch_size)
#imgs = X_train[idx]
#z = encoder1.predict(imgs)
#imgs_ = generator.predict(z)

#z1_gen_ema = encoder1.predict(X_test)
#reconstruct_ema = generator.predict(z1_gen_ema)
#z2_gen_ema = encoder2.predict(reconstruct_ema)

#reconstruct_ema.shape

#predicted_images_10 = (reconstruct_ema *127.5 + 127.5).astype(int)
#X_test_2 = (X_test * 127.5 +127.5).astype(int)

#plt.imshow(X_test_2[318])

#plt.imshow(predicted_images_10[318])

epochs = 200
steps_per_epoch = 200

for epoch in range(epochs +1):
  for steps in range(steps_per_epoch):

    # ---------------------
    #  Train Discriminator
    # ---------------------
    # Select a random batch of images and encode/decode/encode
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    imgs = X_train[idx]
    z = encoder1.predict(imgs)
    imgs_ = generator.predict(z)
    f = get_features.predict(imgs)
    #if steps % 5 == 0:

      # Train the discriminator (imgs are real, imgs_ are fake)
    d_loss_real = discriminator.train_on_batch(imgs, real)
    d_loss_fake = discriminator.train_on_batch(imgs_, fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # ---------------------
    #  Train Generator
    # ---------------------
  
    ## Intermediate outputs and adversarial loss calculation
  
    g_loss = bigan_generator.train_on_batch(imgs, [f, imgs, z])

    g_loss_list.append(g_loss)
    d_loss_list.append(d_loss)

  

  ## saving the predicted and real images at some points during the training
  if epoch % 10 == 0 and epoch > 0:
    encoded_images = encoder1.predict(imgs)
    predicted_temp = generator.predict(encoded_images)
    np.save('/content/drive/My Drive/Thesis_code/thesis_data/predicted_images_ganomaly3/fm_MNIST_real' + str(epoch), imgs)
    np.save('/content/drive/My Drive/Thesis_code/thesis_data/predicted_images_ganomaly3/fm_MNIST' + str(epoch), predicted_temp)


    ## saving the model weights
  if epoch % 2 == 0 and epoch > 0:
    generator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_gen_weights_ganomaly_fm_MNIST.h5')
    encoder1.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc1_weights_ganomaly_fm_MNIST.h5')
    encoder2.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc2_weights_ganomaly_fm_MNIST.h5')
    discriminator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_dis_weights_ganomaly_fm_MNIST.h5')

  ## 
  # Plot the progress
  print("%d [D loss: %f, acc: %.2f%%] [G loss: %f]" %
        (epoch, d_loss[0], 100 * d_loss[1], g_loss[0]))

print(g_loss_list)



print(d_loss_fake)
print(d_loss_real)

plt.plot(np.asarray(g_loss_list)[:, 0], label='G loss')
plt.plot(np.asarray(d_loss_list)[:, 0], label='D loss')
plt.plot(np.asarray(d_loss_list)[:, 1], label='D accuracy')
plt.legend(bbox_to_anchor=(1, 1))

loss_all = np.asarray([np.asarray(g_loss_list)[:, 0],
                        np.asarray(d_loss_list)[:, 0], np.asarray(d_loss_list)[:, 1]])

print(X_test.shape)

## Shuffling the test sets
shuffle_index = np.random.permutation(19567)
X_test_sh = X_test[shuffle_index]
Y_test_sh = Y_test[shuffle_index]
print(X_test_sh.shape)

## Predicting on the test set
z1_gen_ema = encoder1.predict(X_test_sh)
reconstruct_ema = generator.predict(z1_gen_ema)
z2_gen_ema = encoder2.predict(reconstruct_ema)

print(z1_gen_ema.shape)
print(reconstruct_ema.shape)
print(z2_gen_ema.shape)
print(np.min(reconstruct_ema[0]), np.max(reconstruct_ema[0]))

## converting to original scale
predicted_images = reconstruct_ema
predicted_images = (predicted_images *127.5 + 127.5).astype(int)
X_test_2 = (X_test_sh *127.5 +127.5).astype(int)
print(type(predicted_images))
print(predicted_images.shape)

# saving the images
#np.save('/content/drive/My Drive/Thesis_code/thesis_data/predicted_images/predict_np_images', predicted_images)

print(np.min(predicted_images[0]), np.max(predicted_images[0]))

X_test_2 = (X_test_sh *127.5 +127.5).astype(int)
print(np.min(X_test_2[0]), np.max(X_test_2[0]))

## removing axis
predicted_images = np.squeeze(predicted_images, axis=3)
print(predicted_images.shape)

X_test_2 = np.squeeze(X_test_2, axis=3)
print(X_test_2.shape)

## adding axis for plotting images
exp_dim_predicted = np.expand_dims(predicted_images, axis=3)
exp_dim_predicted_r = np.expand_dims(X_test_2, axis=3)
print(exp_dim_predicted.shape)
print(exp_dim_predicted_r.shape)

## calculating the MSE
list_2 = []
for i in range(0, len(X_test_sh)):
  list_2.append(np.mean(np.square(exp_dim_predicted[i] - exp_dim_predicted_r[i]), axis=-1))

np.where(Y_test_sh == 7)

## plots, left (original), middle(generated), right (difference heatmap)
plt.figure(figsize=(15,15))
plt.subplot(1,3,1)
plt.imshow(X_test_2[13], cmap='Greys')
plt.subplot(1,3,2)
plt.imshow(predicted_images[13], cmap='Greys')
plt.subplot(1,3, 3)
plt.imshow(list_2[13])

## calculating anomaly scores
val_list = []
for i in range(0, len(X_test_sh)):
  val_list.append(np.mean(np.square(z1_gen_ema[i] - z2_gen_ema[i])))

anomaly_labels = np.zeros(len(val_list))
for i, label in enumerate(Y_test_sh):
  if label == num_rm:
    anomaly_labels[i] = 1

val_arr = np.asarray(val_list)
val_probs = val_arr / max(val_arr)

"""# Finding Anomaly Scores"""
roc_auc_scores = []
prauc_scores = []

val_arr = np.asarray(val_list)
val_probs = val_arr / max(val_arr)

roc_auc = roc_auc_score(anomaly_labels, val_probs)
prauc = average_precision_score(anomaly_labels, val_probs)
roc_auc_scores.append(roc_auc)
prauc_scores.append(prauc)

print("ROC AUC SCORE FOR %d: %f" % (num_rm, roc_auc))
print("PRAUC SCORE FOR %d: %f" % (num_rm, prauc))

## precision under the recall curve
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from matplotlib import pyplot
from sklearn.metrics import recall_score

f1_y_hat = np.array([])
count_0 = 0
count_1 = 0

for i in val_probs:
  if i > 0.5:
    f1_y_hat = np.append(f1_y_hat, 0)
    count_0 += 1
  else:
    f1_y_hat = np.append(f1_y_hat, 1)
    count_1 += 1

roc_auc_list = []
for remove in range(0, 10):
  anomaly_labels = np.zeros(len(val_arr))
  for j, label in enumerate(Y_test_sh):
    if label == remove:
      anomaly_labels[j] = 1
  roc_auc_list.append(roc_auc_score(anomaly_labels, val_probs))

recall = recall_score(anomaly_labels, f1_y_hat)

print(recall)

