# -*- coding: utf-8 -*-
"""EGBAD_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3YbtMk7g49-Y6aDlXH553f6gKMHvG9W
"""

#mounting drive 
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
## packages
import matplotlib

matplotlib.use('Agg')

from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D, concatenate
from keras.layers import MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.initializers import RandomNormal



from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

## Loading the data
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

## stacking the datasets
X = np.vstack([X_train, X_test])
Y = np.hstack([Y_train, Y_test])

for num_rm in range(10):
  X_non = []
  Y_non = []
  X_rm = []
  Y_rm = []
  for i, label in enumerate(Y):
    if label != num_rm:
      X_non.append(X[i])
      Y_non.append(Y[i])
    else:
      X_rm.append(X[i])
      Y_rm.append(Y[i])

  X_non = np.asarray(X_non)
  X_rm = np.asarray(X_rm)

  X_train, X_test, Y_train, Y_test = train_test_split(X_non, Y_non, 
                                                      test_size=0.2, random_state=42)

  X_test = np.vstack([X_test, X_rm])
  Y_test = np.hstack([Y_test, Y_rm])

print(np.unique(Y_train, return_counts=True))
print(np.unique(Y_test, return_counts=True))

print(X_train.shape)
print(len(Y_train))
print(X_test.shape)
print(len(Y_test))

print(np.min(X_train), np.max(X_train))
print(np.min(X_test), np.max(X_test))

plt.imshow(X_train[5],cmap='Greys')

## Scaling the data between -1 & 1
X_train = (X_train.astype(np.float32) -127.5)/127.5
X_test = (X_test.astype(np.float32) -127.5)/127.5

print(np.min(X_train[10]), np.max(X_train[10]))
print(np.min(X_train), np.max(X_train))

print(np.min(X_test[1000]), np.max(X_test[1000]))
print(np.min(X_test), np.max(X_test))

print(np.max(X_train[80] * 127.5 +127.5))

plt.imshow((X_train[81] * 127.5 +127.5).astype(int), cmap='Greys')

## models
latent_dim = 200
input_shape = (28, 28, 1)

# encoder
e = Sequential()
e.add(Conv2D(32, kernel_size=(3, 3), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(1,1), padding='same', input_shape=input_shape))
e.add(BatchNormalization(momentum=0.8))
e.add(LeakyReLU(alpha=0.1))
#e.add(MaxPooling2D(pool_size=(2,2)))
e.add(Conv2D(64, kernel_size=(3, 3), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2, 2), padding='same'))
e.add(BatchNormalization(momentum=0.8))
e.add(LeakyReLU(alpha=0.1))
#e.add(MaxPooling2D(pool_size=(2,2)))
e.add(Conv2D(128, kernel_size=(3, 3), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2,2), padding='same'))
e.add(BatchNormalization(momentum=0.8))
e.add(LeakyReLU(alpha=0.1))
e.add(Flatten())
e.add(Dense(latent_dim))

e.summary()

img = Input(shape=input_shape)
z = e(img)
encoder = Model(img, z)

encoder.summary()

## Generator

g = Sequential()
g.add(Dense(1024, kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), input_dim=latent_dim))
g.add(BatchNormalization(momentum=0.8))
g.add(Activation('relu'))
g.add(Dense(7* 7* 128))
g.add(BatchNormalization(momentum=0.8))
g.add(Activation('relu'))
g.add(Reshape((7, 7, 128)))
g.add(Conv2DTranspose(64, kernel_size=(4, 4), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2, 2), padding='same'))
g.add(BatchNormalization(momentum=0.8))
g.add(Activation('relu'))
g.add(Conv2DTranspose(1, kernel_size=(4, 4), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2, 2), padding='same', activation='tanh'))

g.summary()

z = Input(shape=(latent_dim,))
gen_img = g(z)
generator = Model(z, gen_img)

generator.summary()

## discriminator
z = Input(shape=(latent_dim,))
img = Input(shape=input_shape)

d = Conv2D(64, kernel_size=(4, 4), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2,2))(img)
d = LeakyReLU(alpha=0.1)(d)
d = Dropout(0.5)(d)
d = Conv2D(64, kernel_size=(4, 4), kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), strides=(2, 2))(d)
d = BatchNormalization(momentum=0.8)(d)
d = LeakyReLU(alpha=0.1)(d)
d = Dropout(0.5)(d)
d = Flatten()(d)

dz = Dense(512, kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02),)(z)
dz = LeakyReLU(alpha=0.1)(dz)
dz = Dropout(0.5)(dz)

d_in = concatenate([d, dz])

full_d = Dense(1024, kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02),)(d_in)
full_d = LeakyReLU(alpha=0.1)(full_d)
full_d = Dropout(0.5)(full_d)

val = Dense(1, kernel_initializer=RandomNormal(mean=0, stddev=0.02), bias_initializer=RandomNormal(mean=0, stddev=0.02), activation='sigmoid')(full_d)

discriminator = Model([z, img], val)

lr_rate = 0.00001
optimizer = Adam(lr_rate, 0.5)

discriminator.compile(loss=['binary_crossentropy'], optimizer=optimizer, metrics=['accuracy'])

discriminator.trainable = False

## encoding image
img = Input(shape=input_shape)
z_ = encoder(img)

## Generated image from z (noise)
z = Input(shape=(latent_dim,))
img_ = generator(z)

fake = discriminator([z, img_])
real = discriminator([z_, img])

egbad = Model([z, img], [fake, real])
egbad.compile(loss=['binary_crossentropy', 'binary_crossentropy'],  
             optimizer=optimizer)

batch_size = 100

real = np.ones((batch_size, 1))
fake = np.zeros((batch_size, 1))

g_loss_list = []
d_loss_list = []

X_train = np.expand_dims(X_train, 3)
X_test = np.expand_dims(X_test, 3)
print(X_train.shape)
print(X_test.shape)

## Load weights
generator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/generator_weights_EGBAD_MNIST.h5')
encoder.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/encoder_weights_EGBAD_MNIST.h5')
discriminator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/discriminator_weights_EGBAD_MNIST.h5')

epochs = 100
steps_per_epoch = 400

for epoch in range(epochs +1):
  for steps in range(steps_per_epoch +1):
    # first round encoder
    index = np.random.randint(0, X_train.shape[0], batch_size)
    imgs = X_train[index]
    z_ = encoder.predict(imgs)
  
    ## first round generator
    z = np.random.normal(size=(batch_size, latent_dim))
    imgs_ = generator.predict(z)

    ## training the generator
    g_loss = egbad.train_on_batch([z, imgs], [real, fake])


  
    d_loss_rl = discriminator.train_on_batch([z_, imgs], real)
    d_loss_fake = discriminator.train_on_batch([z, imgs_], fake)
    d_loss = 0.5 * np.add(d_loss_rl, d_loss_fake)
## saving generated images

  if epoch % 10 == 0 and epoch > 0:
    z = np.random.normal(size=(batch_size, latent_dim))
    imgs_ = generator.predict(z)
    np.save('/content/drive/My Drive/Thesis_code/thesis_data/predicted_images_EGBAD/MNIST_real' + str(epoch), imgs_)
  
  ## saving the models weights
  if epoch % 2 == 0 and epoch > 0:
    encoder.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/encoder_weights_EGBAD_MNIST.h5')
    generator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/generator_weights_EGBAD_MNIST.h5')
    discriminator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/discriminator_weights_EGBAD_MNIST.h5')
  
  
  print("%d [D loss: %f, acc: %.2f%%] [G loss: %f]" %(epoch, d_loss[0], 100*d_loss[1], g_loss[0]))

plt.plot(np.asarray(g_loss_list)[:, 0], label='G loss')
plt.plot(np.asarray(d_loss_list)[:, 0], label='D loss')
plt.plot(np.asarray(d_loss_list)[:, 1], label='D accuracy')
plt.legend(bbox_to_anchor=(1, 1))

## Shuffling the test sets
shuffle_index = np.random.permutation(19567)
X_test_sh = X_test[shuffle_index]
Y_test_sh = Y_test[shuffle_index]
print(X_test_sh.shape)

## Predicting on the test set
z1_gen_ema = encoder.predict(X_test_sh)
reconstruct_ema = generator.predict(z1_gen_ema)


print(z1_gen_ema.shape)
print(reconstruct_ema.shape)
print(np.min(reconstruct_ema[0]), np.max(reconstruct_ema[0]))

## converting to original scale
predicted_images = reconstruct_ema
predicted_images = (predicted_images *127.5 + 127.5).astype(int)
X_test_2 = (X_test_sh *127.5 +127.5).astype(int)
print(type(predicted_images))
print(predicted_images.shape)

print(np.min(predicted_images[0]), np.max(predicted_images[0]))

X_test_2 = (X_test_sh *127.5 +127.5).astype(int)
print(np.min(X_test_2[0]), np.max(X_test_2[0]))

## removing axis
predicted_images = np.squeeze(predicted_images, axis=3)
print(predicted_images.shape)

X_test_2 = np.squeeze(X_test_2, axis=3)
print(X_test_2.shape)

## adding axis for plotting images
exp_dim_predicted = np.expand_dims(predicted_images, axis=3)
exp_dim_predicted_r = np.expand_dims(X_test_2, axis=3)
print(exp_dim_predicted.shape)
print(exp_dim_predicted_r.shape)

## calculating the MSE
list_2 = []
for i in range(0, len(X_test_sh)):
  list_2.append(np.mean(np.square(exp_dim_predicted[i] - exp_dim_predicted_r[i]), axis=-1))

## plots, left (original), middle(generated), right (difference heatmap)
plt.figure(figsize=(15,15))
plt.subplot(1,3,1)
plt.imshow(X_test_2[5], cmap='Greys')
plt.subplot(1,3,2)
plt.imshow(predicted_images[5], cmap='Greys')
plt.subplot(1,3, 3)
plt.imshow(list_2[5])

## calculating anomaly scores
val_list = []
for i in range(0, len(X_test_sh)):
  val_list.append(np.mean(np.square(z1_gen_ema[i] - z2_gen_ema[i])))

anomaly_labels = np.zeros(len(val_list))
for i, label in enumerate(Y_test_sh):
  if label == num_rm:
    anomaly_labels[i] = 1

val_arr = np.asarray(val_list)
val_probs = val_arr / max(val_arr)

"""# Finding Anomaly Scores"""
roc_auc_scores = []
prauc_scores = []

val_arr = np.asarray(val_list)
val_probs = val_arr / max(val_arr)

roc_auc = roc_auc_score(anomaly_labels, val_probs)
prauc = average_precision_score(anomaly_labels, val_probs)
roc_auc_scores.append(roc_auc)
prauc_scores.append(prauc)

print("ROC AUC SCORE FOR %d: %f" % (num_rm, roc_auc))
print("PRAUC SCORE FOR %d: %f" % (num_rm, prauc))

