# -*- coding: utf-8 -*-
"""EGBAD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlbiPdV8oG5VWibg94POHtQ39_VAGfui
"""

#mounting drive 
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
## packages
import matplotlib

matplotlib.use('Agg')

from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D, concatenate
from keras.layers import MaxPooling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.models import Sequential, Model
from keras.optimizers import Adam

from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

## Loading the data
X_train = np.load('/content/drive/My Drive/Thesis_code/thesis_data/training_test_sets/X_train.npy')
Y_train = np.load('/content/drive/My Drive/Thesis_code/thesis_data/training_test_sets/Y_train.npy')
X_test = np.load('/content/drive/My Drive/Thesis_code/thesis_data/training_test_sets/X_test.npy')
Y_test = np.load('/content/drive/My Drive/Thesis_code/thesis_data/training_test_sets/Y_test.npy')

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

np.unique(Y_test, return_counts=True)

print(type(X_test[0]))

print(np.min(X_train[1001]), np.max(X_train[1001]))
print(np.min(X_test[100]), np.max(X_test[100]))

X_train = X_train*255
X_test = X_test*255

print(np.min(X_train), np.max(X_train))
print(np.min(X_test), np.max(X_test))

## Scaling the data between -1 & 1
X_train = (X_train -127.5)/127.5
X_test = (X_test -127.5)/127.5

print(np.min(X_train[10]), np.max(X_train[10]))
print(np.min(X_train), np.max(X_train))

print(np.min(X_test[1000]), np.max(X_test[1000]))
print(np.min(X_test), np.max(X_test))

print(np.max(X_train[80] * 127.5 +127.5))

plt.imshow((X_train[80] * 127.5 +127.5).astype(int))

plt.imshow((X_test[30] * 127.5 +127.5).astype(int))

## The EGBAD network

latent_dim = 100
input_shape = (128, 128, 3)

## Encoder
modelE = Sequential()
modelE.add(Conv2D(64, kernel_size=(4, 4), padding="same", input_shape=input_shape))
modelE.add(LeakyReLU(alpha=0.2))
modelE.add(MaxPooling2D(pool_size=(2, 2)))
modelE.add(Conv2D(128, kernel_size=(4, 4),  padding="same"))
modelE.add(BatchNormalization(momentum=0.8))
modelE.add(LeakyReLU(alpha=0.2))
modelE.add(MaxPooling2D(pool_size=(2, 2)))
modelE.add(Conv2D(128, kernel_size=(4, 4),  padding="same"))
modelE.add(BatchNormalization(momentum=0.8))
modelE.add(LeakyReLU(alpha=0.2))
modelE.add(MaxPooling2D(pool_size=(2, 2)))
modelE.add(Conv2D(256, kernel_size=(4, 4),  padding="same"))
modelE.add(BatchNormalization(momentum=0.8))
modelE.add(LeakyReLU(alpha=0.2))
modelE.add(MaxPooling2D(pool_size=(2, 2)))
modelE.add(Conv2D(512, kernel_size=(4, 4),  padding="same"))
modelE.add(BatchNormalization(momentum=0.8))
modelE.add(LeakyReLU(alpha=0.2))
#modelE.add(MaxPooling2D(pool_size=(2, 2)))
modelE.add(Conv2D(512, kernel_size=(4, 4), strides=(2, 2), padding="same"))
modelE.add(BatchNormalization(momentum=0.8))
modelE.add(LeakyReLU(alpha=0.2))
modelE.add(Flatten())
modelE.add(Dense(latent_dim))

img = Input(shape=input_shape)
z = modelE(img)
encoder = Model(img, z)

encoder.summary()

modelE.summary()

# Generator
modelG = Sequential()
modelG.add(Dense(1024, input_dim=latent_dim))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(LeakyReLU(alpha=0.2))
modelG.add(Dense(4 * 4 *128))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(LeakyReLU(alpha=0.2))
modelG.add(Reshape((4, 4, 128)))
modelG.add(Conv2DTranspose(512, kernel_size=(4, 4), strides=(2, 2), padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(Activation("relu"))
modelG.add(Conv2DTranspose(512, kernel_size=(4, 4), strides=(2, 2), padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(Activation("relu"))
modelG.add(Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(Activation("relu"))
modelG.add(Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding="same"))
modelG.add(BatchNormalization(momentum=0.8))
modelG.add(Activation("relu"))
modelG.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding="same", activation='tanh'))

z = Input(shape=(latent_dim,))
gen_img = modelG(z)
generator = Model(z, gen_img)

modelG.summary()

generator.summary()

## Discriminator
z = Input(shape=(latent_dim,))
img = Input(shape=input_shape)

modelDx = Conv2D(32, kernel_size=(4, 4), strides=(2,2), padding='same')(img)
modelDx = LeakyReLU(alpha=0.1)(modelDx)
modelDx = Dropout(0.5)(modelDx)
modelDx = Conv2D(64, kernel_size=(4, 4), strides=(2,2), padding='same')(modelDx)
modelDx = BatchNormalization(momentum=0.8)(modelDx)
modelDx = LeakyReLU(alpha=0.1)(modelDx)
modelDx = Conv2D(128, kernel_size=(4, 4), strides=(2,2), padding='same')(modelDx)
modelDx = BatchNormalization(momentum=0.8)(modelDx)
modelDx = LeakyReLU(alpha=0.1)(modelDx)
modelDx = Conv2D(256, kernel_size=(4, 4), strides=(2,2), padding='same')(modelDx)
modelDx = BatchNormalization(momentum=0.8)(modelDx)
modelDx = LeakyReLU(alpha=0.1)(modelDx)
modelDx = Dropout(0.5)(modelDx)
modelDx = Flatten()(modelDx)

modelDz = Dense(512)(z)
modelDz = LeakyReLU(alpha=0.2)(modelDz)
modelDz = Dropout(0.5)(modelDz)

d_in = concatenate([modelDx, modelDz])

modelD = Dense(1024)(d_in)
modelD = LeakyReLU(alpha=0.2)(modelD)
modelD = Dropout(0.5)(modelD)

dis_fm = Model([z, img], modelD)
validity = Dense(1, activation="sigmoid")(modelD)

discriminator = Model([z, img], validity)

dis_fm.summary()

discriminator.summary()

learn_rate = 0.00001

optimizer = Adam(learn_rate, 0.5)

# Build and compile the discriminator
discriminator.compile(loss=['binary_crossentropy'],
                              optimizer=optimizer,
                              metrics=['accuracy'])

discriminator.trainable = False

# Generate image from sampled noise
z = Input(shape=(latent_dim,))
img_ = generator(z)

# Encode image
img = Input(shape=input_shape)
z_ = encoder(img)

# Latent -> img is fake, and img -> latent is valid
fake = discriminator([z, img_])
real = discriminator([z_, img])

# Set up and compile the combined model
# Trains generator to fool the discriminator
bigan_generator = Model([z, img], [fake, real])
bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],
                                optimizer=optimizer)

batch_size = 100
epochs = 100
steps_per_epoch = 200

# Adversarial ground truths
real = np.ones((batch_size, 1))
fake = np.zeros((batch_size, 1))

g_loss_list = []
d_loss_list = []

## loading weights to return training process
generator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_gen_weights_EGBAD_ganomaly.h5')
encoder.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc_weights_EGBAD_ganomaly.h5')
discriminator.load_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_dis_weights_EGBAD_ganomaly.h5')

z = np.random.normal(size=(batch_size, latent_dim))
imgs_ = generator.predict(z)

predicted_images_10 = (imgs_ *127.5 + 127.5).astype(int)

plt.imshow(predicted_images_10[9])

X_test_2 = (X_test *127.5 + 127.5).astype(int)
reconstruct_ema_2 = (reconstruct_ema *127.5 + 127.5).astype(int)

plt.imshow(X_test_2[61])

plt.imshow(reconstruct_ema_2[61])

for epoch in range(epochs):
  for steps in range(steps_per_epoch):

    # Sample noise and generate img
    z = np.random.normal(size=(batch_size, latent_dim))
    imgs_ = generator.predict(z)

    # Select a random batch of images and encode
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    imgs = X_train[idx]
    z_ = encoder.predict(imgs)

    # Train the generator (z -> img is valid and img -> z is is invalid)
    g_loss = bigan_generator.train_on_batch([z, imgs], [real, fake])

  
    # Train the discriminator (img -> z is valid, z -> img is fake)
    d_loss_real = discriminator.train_on_batch([z_, imgs], real)
    d_loss_fake = discriminator.train_on_batch([z, imgs_], fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    g_loss_list.append(g_loss)
    d_loss_list.append(d_loss)


    # If at save interval => save generated image samples
  if epoch % 10 == 0 and epoch > 0:
    z = np.random.normal(size=(1, latent_dim))
    rand_img = generator.predict(z)
    np.save('/content/drive/My Drive/Thesis_code/thesis_data/predicted_images/EGBAD_img' + str(epoch), rand_img)

  if epoch % 2 == 0 and epoch > 0:
    generator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_gen_weights_EGBAD_ganomaly.h5')
    encoder.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_enc_weights_EGBAD_ganomaly.h5')
    discriminator.save_weights('/content/drive/My Drive/Thesis_code/thesis_data/models/d_dis_weights_EGBAD_ganomaly.h5')


  # Plot the progress
  print ("%d [D loss: %f, acc: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))

plt.plot(np.asarray(g_loss_list)[:, 0], label='G loss')
plt.plot(np.asarray(d_loss_list)[:, 0], label='D loss')
plt.plot(np.asarray(d_loss_list)[:, 1], label='D accuracy')
plt.legend(bbox_to_anchor=(1, 1))

loss_all = np.asarray([np.asarray(g_loss_list)[:, 0], np.asarray(d_loss_list)[:, 0], np.asarray(d_loss_list)[:, 1]])

z_gen_ema = encoder.predict(X_test)
reconstruct_ema = generator.predict(z_gen_ema)
l_generator_ema = discriminator.predict([z_gen_ema, reconstruct_ema])

print(reconstruct_ema.shape)

import tensorflow as tf

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

# Cross entropy

dis_score = tf.nn.sigmoid_cross_entropy_with_logits(
labels=tf.ones_like(l_generator_ema), logits=l_generator_ema)

dis_score_val = sess.run(dis_score)

weight = 0.9
val_list = []

for i in range(0, len(X_test)):
  # Finding the anomaly score
  delta = X_test[i] - reconstruct_ema[i]
  delta_flat = np.ndarray.flatten(delta)
  gen_score = np.linalg.norm(delta_flat)

  # Total anomaly score
  value = (1 - weight) * gen_score + weight * dis_score_val[i][0]
  val_list.append(value)

roc_auc_scores = []
prauc_scores = []

val_arr = np.asarray(val_list)
val_probs = val_arr / max(val_arr)

roc_auc = roc_auc_score(Y_test, val_probs)
prauc = average_precision_score(Y_test, val_probs)
roc_auc_scores.append(roc_auc)
prauc_scores.append(prauc)

print("ROC AUC SCORE FOR %d: %f" % (1, roc_auc))
print("PRAUC SCORE FOR %d: %f" % (1, prauc))

